{
  "name": "race_based_med",
  "description": "RaceBias is a benchmark used to evaluate language models for racially biased orinappropriate content in medical question-answering scenarios. Each instance consistsof a medical question and a model-generated response. The task is to classify whetherthe response contains race-based, harmful, or inaccurate content. This benchmarksupports research into bias detection and fairness in clinical AI systems.",
  "tags": [
    "knowledge",
    "reasoning",
    "biomedical"
  ],
  "definition_path": "https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/race_based_med_scenario.py"
}