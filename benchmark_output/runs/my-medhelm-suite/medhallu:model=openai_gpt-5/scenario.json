{
  "name": "medhallu",
  "description": "MedHallu is a benchmark focused on evaluating factual correctness in biomedicalquestion answering. Each instance contains a PubMed-derived knowledge snippet, abiomedical question, and a model-generated answer. The task is to classify whether theanswer is factually correct or contains hallucinated (non-grounded) information. Thisbenchmark is designed to assess the factual reliability of medical language models.",
  "tags": [
    "knowledge",
    "reasoning",
    "biomedical"
  ],
  "definition_path": "https://github.com/stanford-crfm/helm/blob/main/src/helm/benchmark/scenarios/medhallu_scenario.py"
}