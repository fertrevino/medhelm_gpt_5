{
  "title": "MedDialog",
  "header": [
    {
      "value": "Model",
      "markdown": false,
      "metadata": {}
    },
    {
      "value": "Jury Score",
      "description": "MedDialog is a benchmark of real-world doctor-patient conversations focused on health-related concerns and advice. Each dialogue is paired with a one-sentence summary that reflects the core patient question or exchange. The benchmark evaluates a model's ability to condense medical dialogue into concise, informative summaries.\n\nMedDialog Jury Score: Measures the average score assigned by an LLM-based jury evaluating task performance.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Jury Score",
        "run_group": "MedDialog"
      }
    },
    {
      "value": "Observed inference time (s)",
      "description": "MedDialog is a benchmark of real-world doctor-patient conversations focused on health-related concerns and advice. Each dialogue is paired with a one-sentence summary that reflects the core patient question or exchange. The benchmark evaluates a model's ability to condense medical dialogue into concise, informative summaries.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "MedDialog"
      }
    },
    {
      "value": "# eval",
      "description": "MedDialog is a benchmark of real-world doctor-patient conversations focused on health-related concerns and advice. Each dialogue is paired with a one-sentence summary that reflects the core patient question or exchange. The benchmark evaluates a model's ability to condense medical dialogue into concise, informative summaries.\n\n# eval: Number of evaluation instances.",
      "markdown": false,
      "metadata": {
        "metric": "# eval",
        "run_group": "MedDialog"
      }
    },
    {
      "value": "# train",
      "description": "MedDialog is a benchmark of real-world doctor-patient conversations focused on health-related concerns and advice. Each dialogue is paired with a one-sentence summary that reflects the core patient question or exchange. The benchmark evaluates a model's ability to condense medical dialogue into concise, informative summaries.\n\n# train: Number of training instances (e.g., in-context examples).",
      "markdown": false,
      "metadata": {
        "metric": "# train",
        "run_group": "MedDialog"
      }
    },
    {
      "value": "truncated",
      "description": "MedDialog is a benchmark of real-world doctor-patient conversations focused on health-related concerns and advice. Each dialogue is paired with a one-sentence summary that reflects the core patient question or exchange. The benchmark evaluates a model's ability to condense medical dialogue into concise, informative summaries.\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
      "markdown": false,
      "metadata": {
        "metric": "truncated",
        "run_group": "MedDialog"
      }
    },
    {
      "value": "# prompt tokens",
      "description": "MedDialog is a benchmark of real-world doctor-patient conversations focused on health-related concerns and advice. Each dialogue is paired with a one-sentence summary that reflects the core patient question or exchange. The benchmark evaluates a model's ability to condense medical dialogue into concise, informative summaries.\n\n# prompt tokens: Number of tokens in the prompt.",
      "markdown": false,
      "metadata": {
        "metric": "# prompt tokens",
        "run_group": "MedDialog"
      }
    },
    {
      "value": "# output tokens",
      "description": "MedDialog is a benchmark of real-world doctor-patient conversations focused on health-related concerns and advice. Each dialogue is paired with a one-sentence summary that reflects the core patient question or exchange. The benchmark evaluates a model's ability to condense medical dialogue into concise, informative summaries.\n\n# output tokens: Actual number of output tokens.",
      "markdown": false,
      "metadata": {
        "metric": "# output tokens",
        "run_group": "MedDialog"
      }
    }
  ],
  "rows": [
    [
      {
        "value": "GPT 5",
        "description": "",
        "markdown": false
      },
      {
        "value": 1.0,
        "description": "min=1, mean=1, max=1, sum=2 (2)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "med_dialog,subset=healthcaremagic:model=openai_gpt-5",
          "med_dialog,subset=icliniq:model=openai_gpt-5"
        ]
      },
      {
        "value": 6.872510290145874,
        "description": "min=6.421, mean=6.873, max=7.324, sum=13.745 (2)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "med_dialog,subset=healthcaremagic:model=openai_gpt-5",
          "med_dialog,subset=icliniq:model=openai_gpt-5"
        ]
      },
      {
        "value": 10.0,
        "description": "min=10, mean=10, max=10, sum=20 (2)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "med_dialog,subset=healthcaremagic:model=openai_gpt-5",
          "med_dialog,subset=icliniq:model=openai_gpt-5"
        ]
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (2)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "med_dialog,subset=healthcaremagic:model=openai_gpt-5",
          "med_dialog,subset=icliniq:model=openai_gpt-5"
        ]
      },
      {
        "value": 0.0,
        "description": "min=0, mean=0, max=0, sum=0 (2)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "med_dialog,subset=healthcaremagic:model=openai_gpt-5",
          "med_dialog,subset=icliniq:model=openai_gpt-5"
        ]
      },
      {
        "value": 314.4,
        "description": "min=248.3, mean=314.4, max=380.5, sum=628.8 (2)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "med_dialog,subset=healthcaremagic:model=openai_gpt-5",
          "med_dialog,subset=icliniq:model=openai_gpt-5"
        ]
      },
      {
        "value": 65.1,
        "description": "min=63.3, mean=65.1, max=66.9, sum=130.2 (2)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "med_dialog,subset=healthcaremagic:model=openai_gpt-5",
          "med_dialog,subset=icliniq:model=openai_gpt-5"
        ]
      }
    ]
  ],
  "links": [
    {
      "text": "LaTeX",
      "href": "benchmark_output/runs/my-medhelm-suite/groups/latex/med_dialog_med_dialog.tex"
    },
    {
      "text": "JSON",
      "href": "benchmark_output/runs/my-medhelm-suite/groups/json/med_dialog_med_dialog.json"
    }
  ],
  "name": "med_dialog"
}