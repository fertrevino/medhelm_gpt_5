{
  "title": "Efficiency",
  "header": [
    {
      "value": "Model",
      "markdown": false,
      "metadata": {}
    },
    {
      "value": "Mean win rate",
      "description": "How many models this model outperforms on average (over columns).",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {}
    },
    {
      "value": "PubMedQA - Observed inference time (s)",
      "description": "PubMedQA is a biomedical question-answering dataset that evaluates a model's ability to interpret scientific literature. It consists of PubMed abstracts paired with yes/no/maybe questions derived from the content. The benchmark assesses a model's capability to reason over biomedical texts and provide factually grounded answers.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "PubMedQA"
      }
    },
    {
      "value": "EHRSQL - Observed inference time (s)",
      "description": "EHRSQL is a benchmark designed to evaluate models on generating structured queries for clinical research. Each example includes a natural language question and a database schema, and the task is to produce an SQL query that would return the correct result for a biomedical research objective. This benchmark assesses a model's understanding of medical terminology, data structures, and query construction.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "EHRSQL"
      }
    },
    {
      "value": "BMT-Status - Observed inference time (s)",
      "description": "BMT-Status is a benchmark composed of clinical notes and associated binary questions related to bone marrow transplant (BMT), hematopoietic stem cell transplant (HSCT), or hematopoietic cell transplant (HCT) status. The goal is to determine whether the patient received a subsequent transplant based on the provided clinical documentation.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "BMT-Status"
      }
    },
    {
      "value": "RaceBias - Observed inference time (s)",
      "description": "RaceBias is a benchmark used to evaluate language models for racially biased or inappropriate content in medical question-answering scenarios. Each instance consists of a medical question and a model-generated response. The task is to classify whether the response contains race-based, harmful, or inaccurate content. This benchmark supports research into bias detection and fairness in clinical AI systems.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "RaceBias"
      }
    },
    {
      "value": "N2C2-CT - Observed inference time (s)",
      "description": "A dataset that provides clinical notes and asks the model to classify whether the patient is a valid candidate for a provided clinical trial.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "N2C2-CT"
      }
    },
    {
      "value": "MedHallu - Observed inference time (s)",
      "description": "MedHallu is a benchmark focused on evaluating factual correctness in biomedical question answering. Each instance contains a PubMed-derived knowledge snippet, a biomedical question, and a model-generated answer. The task is to classify whether the answer is factually correct or contains hallucinated (non-grounded) information. This benchmark is designed to assess the factual reliability of medical language models.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "MedHallu"
      }
    }
  ],
  "rows": [
    [
      {
        "value": "GPT 5",
        "description": "",
        "markdown": false
      },
      {
        "markdown": false
      },
      {
        "value": 4.883398771286011,
        "description": "min=4.883, mean=4.883, max=4.883, sum=4.883 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "pubmed_qa:model=openai_gpt-5"
        ]
      },
      {
        "value": 30.93964045512967,
        "description": "min=30.94, mean=30.94, max=30.94, sum=30.94 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "ehr_sql:model=openai_gpt-5"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 6.473990969657898,
        "description": "min=6.474, mean=6.474, max=6.474, sum=6.474 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "race_based_med:model=openai_gpt-5"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 8.228727684020996,
        "description": "min=8.229, mean=8.229, max=8.229, sum=8.229 (1)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "medhallu:model=openai_gpt-5"
        ]
      }
    ]
  ],
  "links": [
    {
      "text": "LaTeX",
      "href": "benchmark_output/runs/my-medhelm-suite/groups/latex/medical_research_efficiency.tex"
    },
    {
      "text": "JSON",
      "href": "benchmark_output/runs/my-medhelm-suite/groups/json/medical_research_efficiency.json"
    }
  ],
  "name": "efficiency"
}