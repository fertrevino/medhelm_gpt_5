[
  {
    "title": "",
    "header": [
      {
        "value": "Model",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "MedecFlagAcc",
        "description": "Medec is a benchmark composed of clinical narratives that include either correct documentation or medical errors. Each entry includes sentence-level identifiers and an associated correction task. The model must review the narrative and either identify the erroneous sentence and correct it, or confirm that the text is entirely accurate [(Abacha et al., 2025)](https://arxiv.org/abs/2412.19260).\n\nMedical Error Flag Accuracy: Measures how accurately the model identifies whether a clinical note contains an error (binary classification of correct/incorrect).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "MedecFlagAcc",
          "run_group": "Medec"
        }
      },
      {
        "value": "Observed inference time (s)",
        "description": "Medec is a benchmark composed of clinical narratives that include either correct documentation or medical errors. Each entry includes sentence-level identifiers and an associated correction task. The model must review the narrative and either identify the erroneous sentence and correct it, or confirm that the text is entirely accurate [(Abacha et al., 2025)](https://arxiv.org/abs/2412.19260).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Observed inference time (s)",
          "run_group": "Medec"
        }
      },
      {
        "value": "# eval",
        "description": "Medec is a benchmark composed of clinical narratives that include either correct documentation or medical errors. Each entry includes sentence-level identifiers and an associated correction task. The model must review the narrative and either identify the erroneous sentence and correct it, or confirm that the text is entirely accurate [(Abacha et al., 2025)](https://arxiv.org/abs/2412.19260).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "Medec"
        }
      },
      {
        "value": "# train",
        "description": "Medec is a benchmark composed of clinical narratives that include either correct documentation or medical errors. Each entry includes sentence-level identifiers and an associated correction task. The model must review the narrative and either identify the erroneous sentence and correct it, or confirm that the text is entirely accurate [(Abacha et al., 2025)](https://arxiv.org/abs/2412.19260).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "Medec"
        }
      },
      {
        "value": "truncated",
        "description": "Medec is a benchmark composed of clinical narratives that include either correct documentation or medical errors. Each entry includes sentence-level identifiers and an associated correction task. The model must review the narrative and either identify the erroneous sentence and correct it, or confirm that the text is entirely accurate [(Abacha et al., 2025)](https://arxiv.org/abs/2412.19260).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "Medec"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "Medec is a benchmark composed of clinical narratives that include either correct documentation or medical errors. Each entry includes sentence-level identifiers and an associated correction task. The model must review the narrative and either identify the erroneous sentence and correct it, or confirm that the text is entirely accurate [(Abacha et al., 2025)](https://arxiv.org/abs/2412.19260).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "Medec"
        }
      },
      {
        "value": "# output tokens",
        "description": "Medec is a benchmark composed of clinical narratives that include either correct documentation or medical errors. Each entry includes sentence-level identifiers and an associated correction task. The model must review the narrative and either identify the erroneous sentence and correct it, or confirm that the text is entirely accurate [(Abacha et al., 2025)](https://arxiv.org/abs/2412.19260).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "Medec"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "GPT 5",
          "description": "",
          "href": "?group=medec&subgroup=&runSpecs=%5B%22medec%3Amodel%3Dopenai_gpt-5%22%5D",
          "markdown": false,
          "run_spec_names": [
            "medec:model=openai_gpt-5"
          ]
        },
        {
          "value": 0.66,
          "description": "min=0.66, mean=0.66, max=0.66, sum=0.66 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 28.271135165691376,
          "description": "min=28.271, mean=28.271, max=28.271, sum=28.271 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 100.0,
          "description": "min=100, mean=100, max=100, sum=100 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 300.93,
          "description": "min=300.93, mean=300.93, max=300.93, sum=300.93 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 18.28,
          "description": "min=18.28, mean=18.28, max=18.28, sum=18.28 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/my-medhelm-suite/groups/latex/medec_medec_.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/my-medhelm-suite/groups/json/medec_medec_.json"
      }
    ],
    "name": "medec_"
  }
]